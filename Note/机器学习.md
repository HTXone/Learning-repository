# 机器学习

## 性能检验

### 测试集获得

留出法：在训练集中拆分出一部分样本集作为测试集 划分过程中保证数据分布的一致性

交叉验证法：将数据集划分为互斥子集 且每个子集保持数据分布的一致性 每次用k-1个子集作为训练集  剩余子集作为测试集 

自助法：对包含m个样本的数据集D 对其进行采样产生数据集D‘（每次随机从D中抽取一个样本 拷贝放入D'中 然后将该样本放回数据集中 重复m次后得到包含m个样本的测试数据集D'） m足够大的情况下有1/3数据不出现在D'中， D'作为数据集 D\D’作为测试集 适用于 数据集小 难以划分训练和测试集 可产生多个不同的训练集 有利于集成训练 但改变了数据集的样本分布 引入了估计误差

### 性能度量

错误率和精确度

查准率，查全率:
二分类问题混淆矩阵

| 情况\|预测 | 正例         | 反例         |
| ---------- | ------------ | ------------ |
| 正例       | TP(真正例)   | FN（假反例） |
| 反例       | FP（假正例） | TN（真反例） |

查准率：P = TP/(TP+FP)
查全率：R = TP/(TP+FN)
F1度量：(2\*P\*R)/(P+R)

### 代价敏感错误率

不同类型错误的损失不同 代价不平等

### 假设检验：

根据测试错误率来推断泛化错误率

基于二项分布的二项检验
交叉验证t检验，McNemar检验：对两个学习机的性能差进行检验
Friedman检验：多算法性能检验
Nemenyi后续检验：区分性能相差过大算法

### 偏差方差分解：

解释学习算法泛用性能的工具
泛化误差 = 偏差+方差+噪声
偏差：预测和真实结果的偏离程度
方差：同大小训练集变动导致的学习性能的变化 刻画了数据扰动对学习算法造成的影响
噪声：当前任务任何学习算法所能达到的期望泛化误差的下界 即问题本身的难度
泛用性能由算法学习能力 数据充分性 学习任务本身难度共同决定的

## 线性模型

试图通过学得一个通过属性的线性组合来进行预测的函数

### 参数估计

通过模型的最小二乘法来查询一条直线进行函数模拟
也可通过矩阵最小二乘来查询一条曲线完成函数模拟
广义线性回归：通过对数运算等完成输入空间到输出空间的非线性函数映射

### 对数几率回归

通过单位阶跃函数完成从线性回归模型产生的预测实值到0/1间概率值完成二分类问题。
Sigmoid函数

### 极大似然估计法：

极大似然估计法就是要选取这样的t值作为参数t的估计值，使所选取的样本在被选的总体中出现的可能性为最大

### 线性判别分析LDA

给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近 异类样例的投影点尽可能远离
当两类数据同先验，满足高斯分布，且协方差相等时，LDA可达到最优分类

### 多分类学习

使用多二分类分类器对一个已拆分或将拆分为多个二分类任务的多分类任务进行整合
分类问题拆分策略：一对一（OvO) 一对其余(OvR) 多对多（MvM）

### 类别不平衡问题

分类任务中不同类别的训练眼里数目相差很大的情况
再缩放思想：

1. 对训练集内较多样例集进行欠采样，去除一些多余样例保证正反两样例集大小接近
2. 过采样 增加一些较少样例集样例后再进行学习 代表算法：SMOTE
3. 基于原始训练集学习 但在决策过程中使用阈值移动方法

## 决策树

基于树型结构进行决策，叶结点对应决策结果，父结点对应属性测试，目的是产生以可泛用能力强，对未见示例能力强的决策模型。

### 决策划分：

信息熵：度量样本集合纯度最常见的指标

信息增益：样本集在某个属性上进行划分所得到的信息纯度提升的程度，信息增益越大，得到的信息纯度提升越大，即可通过信息增益来进行决策树的划分属性选择。

增益率：样本集在某个属性上进行划分所得到分类数量的大小，增益性越大，划分样本集数量越小。一般通过找出平均信息增益再从中选择增益率高的进行划分。

基尼指数：反映从数据集中随机抽取两个样本，其类别标记不一致的概率，基尼指数越小，数据集纯度越高。

剪枝处理：

预剪枝：决策树生成过程中对每个结点划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升 则停止划分当前结点并标记为叶结点。提前剪枝可能导致欠拟合

后剪枝：先从训练集生成一棵完整的决策树，然后自底向上对非叶结点进行考察，若将该节点队员子树替换为叶结点能带来决策树性能的提升，则将该子树替换为叶结点。
（通过性能评估方法对性能是否提升做出判断）避免欠拟合现象但会加大训练时间

### 连续值与缺失值

连续值处理：1.设置划分点，将连续值问题转为分类问题进行处理。可使用信息增益算法来确认划分点。

缺失值处理：（属性下数值缺失）处理两个问题：如何在属性值缺失情况下进行划分属性选择，给定划分属性若样本在该属性上的值缺失将如何对样本进行划分。
对于第一个问题，通过对缺失属性样本集在数据集的补集来确定属性划分依据。
对于第二个问题，将样本同时划分到两个子节点中。

### 多变量决策树：

决策树不再是简单的二叉树而是多分支树。非叶结点是对属性的线性组合进行测试从而试图建立一个合适的线性分类器





# 李宏毅2020

## 回归函数预测：

1. 预设多个不同参数项的线性模型
2. 使用损失函数来计算每个线性模型的好坏

梯度下降：对可微分方程进行最佳参数选取
方法：随机选取一些参数来初始化，对已选参数进行求导计算，通过微分值或偏微分值来判断函数下降方向并与预设训练步长一起决定训练幅度

在线性函数中没有局部最优解

正则化：对损失函数进行正则化使函数曲线更平滑，使得对噪声进行一定的屏蔽

损失来自偏方和方差
偏差：预测结果平均值与结果标签的偏移
方差：预测结果的分散性

偏差大导致欠拟合 优化模型
方差大导致过拟合 增加更多数据来学习，正则化

梯度下降，通过求高斯平面来获取最优解梯度方向

步长适应：通过调整步长来训练模型
AdaGrad：单调递减学习率 加入二次微分来对多维线性函数来对训练步长进行优化
StochasticGrad：随机梯度下降 对每个训练例来进行步长适应 相较于原学习所有例子后再进行loss学习加快学习速度和随机性

特征缩放：尽量让每个特征的缩放图相同 使尽量在梯度图上以一种较为平滑的直线到达最优解
方法：计算平均值和标准偏差
![image-20201019100825124](C:\Users\79931\AppData\Roaming\Typora\typora-user-images\image-20201019100825124.png)

梯度下降：
通过泰勒展开简化loss函数 然后通过简化后函数来计算区域最小值 获得下降方向
<img src="D:\Data\Learn\QQ截图20201019102519.png" style="zoom:67%;" />
<img src="D:\Data\Learn\QQ截图20201019102646.png" style="zoom:50%;" />

为保证泰勒简化函数的正确性 需要学习步长足够小
一般梯度下降导致局部最小梯度出现

### 优化方法

找到最优模型参数使loss值最小

SGD：随机梯度下降
SGDwithMomentum：动量随机梯度下降 加入上次下降方向对下降发现的影响 防止局部最优解的影响（收敛稳定）
RMSProp：对AdaGrad算法平方梯度进行增加衰减系数来优化
Adam：SGDM和RMSProp的融合优化（收敛快）
SWATS：开始训练使用Adam 训练到一定准确率后切换为SGDM
AMSGrad：降低Adam训练中无意义梯度的影响 只处理大的训练步长
AdaBound：
Cyclical LR：SGDM的learningRate取值问题
RAdam：Adam的优化 自动调节学习步长 开始时使用SGDM 稳定后使用Adam
Lookahead：保持训练的平稳性
NAG：SGDM预测优化
AdamW：权重衰减和L2正则化
<img src="D:\Data\Learn\QQ截图20201019181454.png" style="zoom:50%;" /><img src="D:\Data\Learn\QQ截图20201019181529.png" style="zoom:50%;" />
<img src="D:\Data\Learn\QQ截图20201019181648.png" style="zoom:50%;" />

对训练集变化来进行优化：
Shuffling：每次随机输入训练集
Dropout：丢弃某些训练例
Gradient noise：增加梯度噪声
slot filing ：槽位填充， 序列标注的一种

BERT模型：自然语言处理模型
YOLO：图形鉴别模型
Mask R-CNN：目标识别模型
ResNet：残差网络
big-Gan：图片生成对抗网络
MAML：模型无关元学习算法 适用于对逐渐新加入分类任务的学习
RNN：循环神经网络
CNN: 卷积神经网络
LSMT：长短记忆神经网络
Fast-RCNN: 目标识别模型
Faster-RCNN:目标识别
R-FCN：目标识别
FCN：完全神经网络
SSD：目标识别模型
FPN:小目标难检测问题，SSD+FCN+RPN
RPN：区域生成神经网络
Overfeat：特征提取的图形分类
N-gram
NLP:自然语言处理
Keras：开源人工神经网络库
GNN:图神经网络
Grad-CAM：CNN可视化
Smooth Grad：CNN可视化
Layer-wise Relevance propagation(LRP):增强网络可解释性
Guided Backpropagation：导向反向传播 增强网络可解释性

Transformer：神经网络模型<img src="D:\Data\Learn\QQ截图20201026205247.png" style="zoom:33%;" />
DeepWalk：图神经网络算法
node2vec：图神经网络算法
ChebNet：图谱网络
GCN：图卷积网络
Graphsage:GCN分支
SCNN：车道线检测
MLP：多层感知机
GIN：
Diffpool：可微分池化层
GAT：图注意力网络
MoNet：
VAE:
HMM：隐马尔可夫模型 解隐藏参数问题
BERT：
VAE:变分编码器 
FGSM：Fast Gradient Sign Method 白盒线性攻击（需要模型参数） 梯度大致反向训练
Basic iterative method：分类攻击
L-BFGS：分类攻击
JSMA：分类攻击
One Pixel Attack：分类攻击
Carlini and Wagner Attacks C&W：分类攻击
DeepFool：分类攻击
Universal Adversarial Perturbations：生成对任意图像实现攻击的扰动
UPSET and ANGRI：黑箱攻击
Houdini：欺骗基于深度学习的算法的攻击方法
ATNs：前向神经网络攻击
Spatially Transformed：分类攻击

### 分类问题

当作传统线性模型训练时问题：因为过于离开线性模型的数据点出现偏差

理想模型：通过一个函数计算，基于输出结果的正负进行分类
损失函数：对所有分类错误数进行和运算
模型：感知机，支持向量机

极大似然估计：样例特征最有可能出现于哪个类别 通过最可能概率进行分类
参数：类别出现的概率P(C)，样例在类别中出现的概率P(x|C)
样例是某一类别的概率：![](D:\Data\Learn\QQ截图20201020210343.png)

生成模型：通过极大似然估计参数 得到产生样例的几率来生成样例

特征：不同样例的不同属性下的特征值形成的向量

高斯分布：通过高斯分布函数获得某个向量在特征空间中出现的概率 
两个参数：随机变量平均值，随机变量方差

极大似然估计：估算最可能生成关于分类问题的高斯分布的参数值

使用相同方差进行极大似然估计
<img src="D:\Data\Learn\QQ截图20201021101426.png" style="zoom:67%;" />

高斯概率模型：
<img src="D:\Data\Learn\QQ截图20201021101812.png" style="zoom:50%;" />

伯努利分布：是非分类

后验概率：
Sigmoid函数：
<img src="D:\Data\Learn\QQ截图20201021102323.png" style="zoom:67%;" />
<img src="D:\Data\Learn\QQ截图20201021103344.png" style="zoom:67%;" />

由概率模型推导得到学习问题为w和b的学习

### 逻辑回归：

loss函数：
<img src="D:\Data\Learn\QQ截图20201021105722.png" style="zoom:50%;" />
<img src="D:\Data\Learn\QQ截图20201021110207.png" style="zoom:50%;" />

两者同样通过梯度下降来学习参数

逻辑回归下两种损失函数（交叉熵和线性差）的比较：
<img src="D:\Data\Learn\QQ截图20201021111238.png" style="zoom:50%;" />

生成模型和判别模型比较：
生成模型基于均差和方差进行判别
判别模型直接通过训练集寻找W和b来进行训练

生成模型可通过少量训练集来进行推测训练，对噪声的屏蔽性更高，训练集和概率参数可以来自不同的源。

### 多分类：

<img src="D:\Data\Learn\QQ截图20201021181057.png" style="zoom:50%;" />
Softmax：强化值函数

通过多个逻辑回归模型来完成特征处理问题（类神经网络）

## Deep Learning：

隐藏层对输入进行特征提取，输出层通过特征来进行分类
向后传播，通过梯度下降算法更新神经元数值

### 向后传播

![](D:\Data\Learn\QQ截图20201022211342.png)

![](D:\Data\Learn\QQ截图20201022212545.png)
向后传播公式：
<img src="D:\Data\Learn\QQ截图20201022213533.png" style="zoom:50%;" />

deep learning流程
定义模型
定义损失函数来训练模型
选择好的模型

### 对深度学习模型的修改

两个问题：在训练上的结果好坏，在测试上的结果好坏

<img src="D:\Data\Learn\QQ截图20201022214920.png" style="zoom:50%;" />

dropout：在训练时得到好结果但测试上不能得到好结果时使用

 层数过多会导致无法有效更新上层参数
使用ReLU防止多层的梯度消失问题 
Maxout（ReLU的普适版）通过计算来获取合适的激活函数
<img src="D:\Data\Learn\QQ截图20201022221059.png" style="zoom:50%;" />

Early Stopping：
使用验证集验证，当验证集loss已经开始逐渐上升时停止训练。

Regularization（正则化）：
在loss函数上进行改动
<img src="D:\Data\Learn\QQ截图20201023192246.png" style="zoom:67%;" />

L1正则：直接减去多余值（基于某些特殊特征）
L2正则：将多余值进行乘小于1的值（更加平滑）

dropout：基于某个概率来丢掉某些神经元

## 卷积网络：

CNN：用于处理图像或类图像问题，通过小部分特征匹配来获取进行训练

通过embedding dimension来使用cnn学习自然语言处理

GNN：

分类问题
生成问题
![]()![QQ截图20201026224357](D:\Data\Learn\QQ截图20201026224357.png)

通过傅里叶转化等信号变化计算使得图得以进行卷积计算实现特征提取网络

## RNN：

通过memory模块使模型对同样的输入联系上下文获得不同的值
通过循环运行网络来对更新memory的值
![](D:\Data\Learn\QQ截图20201105194914.png)
![](D:\Data\Learn\QQ截图20201105200009.png)

正反向同时更新 上下文分析 
LSTM 持久型短期记忆
<img src="D:\Data\Learn\QQ截图20201105202609.png" style="zoom:50%;" /><img src="D:\Data\Learn\QQ截图20201105203648.png" style="zoom:33%;" />
通过LSTM来训练RNN模型
语义分析 情感分类 摘要分析

CTC：连接时间分类算法 
多对多分析 

Attention-based Model：注意力基础模型      知识图谱
<img src="D:\Data\Learn\QQ截图20201106110629.png" style="zoom:33%;" /><img src="D:\Data\Learn\QQ截图20201106110705.png" style="zoom:33%;" />


## Transformer：

seq2seq 模型  串到串模型转换

使用Self-Attention Layer 来代替RNN处理串转换问题
<img src="D:\Data\Learn\QQ截图20201106172853.png" style="zoom:50%;" />

## Semi-supervised半监督学习:

基于假设模型学习
低密度分离（二分类）

Self-training: 学习有标签数据后通过已学习模型来预测非标签数据 预测完后加入有标签数据
使用强分类标签而不是概率作为标签

Entropy-based Regularization :   熵函数分类
	loss:	<img src="D:\Data\Learn\QQ截图20201107104543.png" style="zoom:50%;" />

Outlook : Semi-supervised SVM
对每个未标签数据进行穷举可能  学习超平面来 得到最大边差 和 最小的已标签数据错误

Smoothness Assumption 平滑度假设：
假设数据集分类的函数平滑度高 即数据集形成一种聚落分布
聚类并进行拟标签 

Graph-based Approach：图联通分类方法

ELMO：Embeddings from Language Model: 语言嵌入式模型
RNN Base

BERT：Bidirectional Encoder Representations from Transformers NLP问题模型
基于Transformers的双向编码模型
<img src="D:\Data\Learn\QQ截图20201107224122.png" style="zoom:50%;" />

ERNIE：Enhanced Representation through Knowledge Integration NLP模型

GPT：Generative Pre-Training NLP问题模型

Explainable ML:可解释原因的机器学习：
解释机器学习为何对问题得出相应答案
将样本分为对象集来进行处理

遮挡法：取消某些对象集内元素来查看是否对预测结果产生影响

偏置法：对对象集内某些元素加上一个偏置 求结果对对象集的微分来判断学习到的特征

通过可解释的模型来模拟拟合不可解释模型的行为
<img src="D:\Data\Learn\QQ截图20201109215835.png" style="zoom:50%;" />

LIME：通过选取样本点来通过学习一个线性函数来解释模型基于哪些样本点进行判断

## 对抗网络：

攻击网络

<img src="D:\Data\Learn\QQ截图20201109222356.png" style="zoom:45%;" />

差异常数计算：
<img src="D:\Data\Learn\QQ截图20201109222530.png" style="zoom:50%;" />

通过在高维平面上特定方向的偏序移动让机器产生辨识错误
<img src="D:\Data\Learn\QQ截图20201110180944.png" style="zoom:33%;" />

白箱攻击：基于网络参数进行攻击
黑箱攻击：基于相同训练数据，先在本地训练相似网络体系后产生攻击集攻击目标网络。

防御网络：
通过平滑法来防止攻击噪声
通过数据处理来防止攻击噪声
基于攻击方法来修补漏洞

## 压缩网络：

选择重要的网络节点，检出不必要的网络节点，再次训练

使用小型网络难训练 所以通过先训练大型网络再进行压缩

压缩方法：减去神经元进行网络压缩

## 知识蒸馏：

通过大网络学习后让小模型学习大模型的预测结果











# 数学

欧几里得
空间：n维空间中所有向量构成的集合称为n维欧几里得空间

子空间

<img src="D:\QQFILE\MobileFile\IMG_20201224_201140.jpg" alt="image-20201224200730464" style="zoom:67%;" />

