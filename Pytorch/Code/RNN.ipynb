{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "import math \n",
    "from mxnet import autograd,nd\n",
    "from mxnet.gluon import loss as gloss\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "#(corpus_indices,char_to_idx,idx_to_char,vocab_size) = d2l.load_data_jay_lyrics()\n",
    "def load_data_jay_lyrics():\n",
    "    \"\"\"Load the Jay Chou lyric data set (available in the Chinese book).\"\"\"\n",
    "    with zipfile.ZipFile('./data/jaychou_lyrics.txt.zip') as zin:\n",
    "        with zin.open('jaychou_lyrics.txt') as f:\n",
    "            corpus_chars = f.read().decode('utf-8')\n",
    "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "    vocab_size = len(char_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size\n",
    "\n",
    "(corpus_indices,char_to_idx,idx_to_char,vocab_size) = load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 1. ... 0. 0. 0.]]\n",
       "<NDArray 2x1027 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.one_hot(nd.array([0,2]),vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将训练集进行onehot编码\n",
    "def to_onehot(X,size):\n",
    "    return [nd.one_hot(x,size) for x in X.T]\n",
    "\n",
    "X = nd.arange(10).reshape((2,5))\n",
    "inputs = to_onehot(X,vocab_size)\n",
    "len(inputs),inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use gpu(0)\n"
     ]
    }
   ],
   "source": [
    "#初始化模型参数\n",
    "num_inputs,num_hiddens,num_outputs = vocab_size,256,vocab_size\n",
    "ctx = d2l.try_gpu()\n",
    "print('will use',ctx)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale=0.01,shape=shape,ctx=ctx)\n",
    "    \n",
    "    #hidden layer\n",
    "    W_xh = _one((num_inputs,num_hiddens))\n",
    "    W_hh = _one((num_hiddens,num_hiddens))\n",
    "    b_h = nd.zeros(num_hiddens,ctx=ctx)\n",
    "    \n",
    "    #output layer\n",
    "    W_hq = _one((num_hiddens,num_outputs))\n",
    "    b_q = nd.zeros(num_outputs,ctx=ctx)\n",
    "    \n",
    "    #grad\n",
    "    params = [W_xh,W_hh,b_h,W_hq,b_q]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "def init_rnn_state(batch_size,num_hiddens,ctx):\n",
    "    return (nd.zeros(shape=(batch_size,num_hiddens),ctx=ctx),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs,state,params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X,W_xh)+nd.dot(H,W_hh)+b_h)  #使用tanh作为激活函数\n",
    "        Y = nd.dot(H,W_hq)+b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs,(H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027), (2, 256))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = init_rnn_state(X.shape[0],num_hiddens,ctx)\n",
    "inputs = to_onehot(X.as_in_context(ctx),vocab_size)\n",
    "params = get_params()\n",
    "outputs,state_new  = rnn(inputs,state,params)\n",
    "len(outputs),outputs[0].shape,state_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix,num_chars,rnn,params,init_rnn_state,\n",
    "                num_hiddens,vocab_size,ctx,idx_to_char,char_to_idx):\n",
    "    state = init_rnn_state(1,num_hiddens,ctx)\n",
    "    #基于前缀prefix（含有数个字符的字符串）预测\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars+len(prefix)-1):\n",
    "        # 将上⼀时间步的输出作为当前时间步的输⼊\n",
    "        X = to_onehot(nd.array([output[-1]],ctx=ctx),vocab_size)\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y,state) = rnn(X,state,params)\n",
    "        # 下⼀个时间步的输⼊是prefix⾥的字符或者当前的最佳预测字符\n",
    "        if t <len(prefix)-1:\n",
    "            output.append(char_to_idx[prefix[t+1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(axis=1).asscalar()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开瑰恩b痛银望杨搞靠星'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n",
    "            ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪裁梯度 防止梯度爆炸\n",
    "def grad_clipping(params,theta,ctx):\n",
    "    norm = nd.array([0],ctx)\n",
    "    for param in params:\n",
    "        norm += (param.grad ** 2).sum()\n",
    "    norm = norm.sqrt().asscalar()\n",
    "    if norm > theta:   #梯度的L2范数不超过theta\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn(rnn,get_params,init_rnn_state,num_hiddens,vocab_size,\n",
    "                        ctx,corpus_indices,idx_to_char,char_to_idx,is_random_iter,\n",
    "                        num_epochs,num_steps,lr,clipping_theta,batch_size,pred_period,\n",
    "                        pred_len,prefixes):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = d2l.data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = d2l.data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:    #相邻采样，epoch开始时初始化隐藏状态\n",
    "            state = init_rnn_state(batch_size,num_hiddens,ctx)\n",
    "        l_sum,n,start = 0.0,0,time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices,batch_size,num_steps,ctx)\n",
    "        for X,Y in data_iter:     #随机采样，在每个小批量更新前初始化隐藏状态\n",
    "            if is_random_iter:\n",
    "                state = init_rnn_state(batch_size,num_hiddens,ctx)\n",
    "            else:    # 否则需要使⽤detach函数从计算图分离隐藏状态\n",
    "                for s in state:\n",
    "                    s.detach()\n",
    "            with autograd.record():\n",
    "                inputs = to_onehot(X,vocab_size)\n",
    "                # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "                (outputs,state) = rnn(inputs,state,params)\n",
    "                # 拼接之后形状为(num_steps * batch_size, vocab_size)\n",
    "                outputs = nd.concat(*outputs,dim = 0)\n",
    "                # Y的形状是(batch_size, num_steps)，转置后再变成⻓度为\n",
    "                # batch * num_steps 的向量，这样跟输出的⾏⼀⼀对应\n",
    "                y = Y.T.reshape((-1,))\n",
    "                # 使⽤交叉熵损失计算平均分类误差\n",
    "                l = loss(outputs,y).mean()\n",
    "            l.backward()\n",
    "            grad_clipping(params,clipping_theta,ctx)\n",
    "            d2l.sgd(params,lr,1)\n",
    "            l_sum+=l.asscalar() * y.size\n",
    "            n+=y.size\n",
    "        if (epoch+1) % pred_period ==0 :\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                    epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(\n",
    "                        prefix, pred_len, rnn, params, init_rnn_state,\n",
    "                        num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '笑了']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 66.469914, time 0.27 sec\n",
      " - 分开 我想要这 你有我 别子我  你想我 别子我  你你 我想要再不 我不要再想 我不 我不 我不 我不\n",
      " - 笑了 我想要再 你有我 别子我  你想我 别子我  你你 我想要再不 我不要再想 我不 我不 我不 我不\n",
      "epoch 100, perplexity 10.270906, time 0.27 sec\n",
      " - 分开 不颗我抬起头 有话去对医药 说你都很有  后悔你的我有多 就多 是 再血我遇多你 说知去的太是 像\n",
      " - 笑了 我不要再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承暴我已无处可躲 我不要再想 我不 我\n",
      "epoch 150, perplexity 2.883388, time 0.25 sec\n",
      " - 分开 爱子在人留 不场掌斗 全家怕日出 白色蜡烛 温暖了空屋 白色蜡烛 温暖了空屋 白色蜡烛 温暖了空屋\n",
      " - 笑了一碗悲粥 配上几斤的牛 它在安老斑鸠 平常话不多 除非都乌的喝了 我说店小二坦三著 别想我有别得到 \n",
      "epoch 200, perplexity 1.580783, time 0.25 sec\n",
      " - 分开 一直在步三在的只斑鸠 印地安老斑鸠 腿短毛不多 几天都没有喝水也能活 脑袋瓜有一点秀逗 猎物死了它\n",
      " - 笑了一碗都粥 配上几斤 是谁在练太极 风生水起 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 习武我有\n",
      "epoch 250, perplexity 1.305877, time 0.25 sec\n",
      " - 分开 那子她娘三仪的母斑鸠 印仔红蕃 在小镇 背对背决斗 一只灰狼 问候完空  偷有梦夫  一了痛夫落 \n",
      " - 笑了一碗热粥 配上几斤的玄 印地安老斑鸠 平常话不多 除非是乌鸦抢了它的窝 它在灌木丛旁邂逅 一只令它心\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                        vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                        char_to_idx, True, num_epochs, num_steps, lr,\n",
    "                        clipping_theta, batch_size, pred_period, pred_len,\n",
    "                        prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 62.487440, time 0.25 sec\n",
      " - 分开 我想要再想 我不要再想 我不能这想 我不要再想 我不能这想 我不能这想 我不能这想 我不能这想 我\n",
      " - 笑了 我想要再想 我不要再想 我不能这想 我不要再想 我不能这想 我不能这想 我不能这想 我不能这想 我\n",
      "epoch 100, perplexity 7.344889, time 0.25 sec\n",
      " - 分开 我想要再想你 你你想这 你天了沙截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快\n",
      " - 笑了 我说要这样 我不要再想 我不能再想 我不 我不 我不要 爱情走的太快就像龙卷 我不能再想 我不要再\n",
      "epoch 150, perplexity 2.115241, time 0.25 sec\n",
      " - 分开 我给想再样你 你静那 一直走 我想就这样牵着你的手不放开 爱可不可以简简单单没有伤害 你 靠着我的\n",
      " - 笑了你都错 别想怎么 你来得难的溪边 默只就待远 说你着始些 我不能再想 我不能再想 我不 我不 我不能\n",
      "epoch 200, perplexity 1.286326, time 0.26 sec\n",
      " - 分开 我说想这样你 爱静不是不人 漂分都靠板球 得分都靠多 除非过乌鸦抢了它的窝 它在灌木丛旁邂逅 一只\n",
      " - 笑了 我说儿 爱情走著看着就 说里手著我后棒 去教就跟已经怎么我 上海一九四三 泛黄的春联还残宙物了有弥\n",
      "epoch 250, perplexity 1.203464, time 0.27 sec\n",
      " - 分开 我说想这样布这样的心不  为了我我想爸 那么凶 如果真 我属于这样牵 你的人美主 你家的美的字了 \n",
      " - 笑了 我说到 如情走著我想要我 我和不口我 你都寄红 不是完美 偷偷出容 没一秒钟 不敢不同 你不懂 连\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                        vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                        char_to_idx, False, num_epochs, num_steps, lr,\n",
    "                        clipping_theta, batch_size, pred_period, pred_len,\n",
    "                        prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
