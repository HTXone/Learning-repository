# Pytorch学习

## NDArray

向量类 运算的单元

## 深度学习基础

线性回归：对变量间的线性关系进行拟合

损失函数：衡量模型预测与真实数据的偏差 在线性回归过程中，希望找到一组模型参数来使训练样本平均损失最小

优化算法：小批量随机梯度下降

• 和⼤多数深度学习模型⼀样，对于线性回归这样⼀种单层神经⽹络，它的基本要素包括模
型、训练数据、损失函数和优化算法。
• 既可以⽤神经⽹络图表⽰线性回归，⼜可以⽤⽮量计算表⽰该模型。
• 应该尽可能采⽤⽮量计算，以提升计算效率

在Gluon中，Sequential实例可以看作是⼀个串联各个层的容器。在构造模型时，我们在该容器中依次添加层。当给定输⼊数据时，容器中的每⼀层将依次计算并将输出作为下⼀层的输⼊。

在Gluon中，data模块提供了有关数据处理的⼯具，nn模块定义了⼤量神经⽹络的层，loss模块定义了各种损失函数

MXNet的initializer模块提供了模型参数初始化的各种⽅法。

softmax回归：预测离散分类问题的相似概率

与线性回归相比，将输出单元从一个变为多个，且引入softmax运算使输出更适合离散值的预测和训练

通过交叉熵来衡量两个概率分布差异 通过最小化交叉熵损失函数来最大化预测概率

• softmax回归适⽤于分类问题。它使⽤softmax运算输出类别的概率分布。
• softmax回归是⼀个单层神经⽹络，输出个数等于分类问题中的类别个数。
• 交叉熵适合衡量两个概率分布的差异。

## 多层感知机

结构：输入层，输出层，隐藏层
神经网络只是引入隐藏层 不加变化 则仅等价于单层神经网络

通过引入非线性的激活函数 改变网络价值

激活函数
ReLU：只保存正数 导数上0不可导，正数为1负数为0
Sigmoid：正数接近1 负数接近0 导数上越接近0导数越大
tanh：正数接近1 负数接近-1 导数上越接近0导数越大 与sigmoid相比更陡峭

## 模型选择：

训练误差与泛化误差：训练误差指模型在训练数据集上表现出的误差，泛化误差指的是任意测试数据集上表现出的误差期望

验证集：预留在训练集和测试集之外的数据进行模型选择（测试集严格上只在最后评价使用，但是一般测试集与验证集的界限模糊）

K折：原始旋连数据分割成k个不同子数据集，用K-1个训练 用1个验证，每次验证子数据集不同 取平均

欠拟合：模型无法得到较低的训练误差

过拟合：训练误差远小于在测试数据集上的误差

模型复杂度越低越容易出现欠拟合 模型复杂度越高 越容易出现过拟合（高阶多项式函数比低阶多项式函数更容易在相同训练数据上得到低的训练误差）

训练数据集大小 在训练集中样本过少时容易出现过拟合

由于⽆法从训练误差估计泛化误差，⼀味地降低训练误差并不意味着泛化误差⼀定会降低。
机器学习模型应关注降低泛化误差。可以使⽤验证数据集来进⾏模型选择。
⽋拟合指模型⽆法得到较低的训练误差，过拟合指模型的训练误差远小于它在测试数据集上的误差。应选择复杂度合适的模型并避免使⽤过少的训练样本。

通过权重衰减来减少过拟合问题(L2范数正则化)

L2范数正则化在模型原损失函数基础上添加L2范数惩罚项，从而得到训练所需要最小化的函数。
L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积。

权重衰减通过惩罚绝对值较⼤的模型参数为需要学习的模型增加了限制

• 正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常⽤⼿
段。
• 权重衰减等价于L2范数正则化，通常会使学到的权重参数的元素较接近0。
• 权重衰减可以通过Gluon的wd超参数来指定。
• 可以定义多个Trainer实例对不同的模型参数使⽤不同的迭代⽅法

通过丢弃法解决过拟合问题

通过概率丢弃层单元从而在训练模型时起到正则化的作⽤，并可以⽤来应对过拟合

但在测试过程中为保障确定的结果启用所有节点

正向传播 正向传播是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）。

反向传播 反向传播指的是计算神经⽹络参数梯度的⽅法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输⼊层的顺序，依次计算并存储⽬标函数有关神经⽹络各层的中间变量以及参数的梯度。

正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。

反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的

在模型参数初始化完成后，我们交替地进⾏正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。

当神经⽹络的层数较多时，模型的数值稳定性容易变差。

解决梯度衰减和爆炸问题：随机初始化模型参数

通过使用Block来实现更复杂的模型

对于使⽤Sequential类构造的神经⽹络，我们可以通过⽅括号[]来访问⽹络的任⼀层

对于Sequential实例中含模型参数的层，我们可以通过Block类的params属性来访问该层包含的所有参数。

Gluon⾥参数类型为Parameter类，它包含参数和梯度的数值，可以分别通过data函数和grad函数来访问。

可以使⽤collect_params函数来获取net变量所有嵌套（例如通过add函数嵌套）
的层所包含的所有参数。它返回的同样是⼀个由参数名称到参数实例的字典。

通过NDArray的context属性来查看该NDArray所在的设备。

如果源变量和⽬标变量的context⼀致，as_in_context函数使⽬标变量和源
变量共享源变量的内存或显存。而copyto函数总是为⽬标变量开新的内存或显存。

## 卷积神经网络

卷积层可通过重复使⽤卷积核有效地表征局部空间。

卷积运算与互相关运算：为了得到卷积运算的输出，我们只需将核数组左右翻转并
上下翻转，再与输⼊数组做互相关运算。

特征图核感受野：⼆维卷积层输出的⼆维数组可以看作是输⼊在空间维度（宽和⾼）上某⼀级的表征，也叫特征图（feature map）。影响元素x的前向计算的所有可能输⼊区域（可能⼤于输⼊的实际尺⼨）叫做x的感受野（receptive field）。

卷积输出：假设输⼊形状是n_h × n_w，卷积核窗口形状是k_h × k_w，那么输出形状将会是
(n_h − k_h + 1) × (n_w − k_w + 1).

padding：在⾼的两侧⼀共填充p_h⾏，在宽的两侧⼀共填充p_w列，那么输出形状将会是
(n_h − k_h + p_h + 1) × (n_w − k_w + p_w + 1),

在很多情况下，我们会设置ph = kh−1和pw = kw −1来使输⼊和输出具有相同的⾼和宽

假设这⾥kh是奇数，我们会在⾼的两侧分别填充ph/2⾏。如果kh是偶数，⼀种可能是在输⼊的顶端⼀侧填充⌈ph/2⌉⾏，而在底端⼀侧填充⌊ph/2⌋⾏。在宽的两侧填充同理。

步幅：当⾼上步幅为s_h，宽上步幅为s_w时，输出形状为
⌊(n_h − k_h + p_h + s_h)/s_h⌋ × ⌊(n_w − k_w + p_w + s_w)/s_w⌋.

如果设置p_h = k_h−1和p_w = k_w −1，那么输出形状将简化为⌊(n_h+s_h−1)/s_h⌋×⌊(n_w +s_w −1)/s_w⌋。更进⼀步，如果输⼊的⾼和宽能分别被⾼和宽上的步幅整除，那么输出形状将是(n_h/s_h)×(n_w/s_w)。

多通道：

假设输⼊数据的通道数为ci，那么卷积核的输⼊通道数同样为ci。设卷积核窗口形状为kh × kw。当ci = 1时，我们知道卷积核只包含⼀个形状为kh × kw的⼆维数组。当ci > 1时，我们将会为每个输⼊通道各分配⼀个形状为kh × kw的核数组。把这ci个数组在输⼊通道维上连结，即得到⼀个形状为ci × kh × kw的卷积核。由于输⼊和卷积核各有ci个通道，我们可以在各个通道上对输⼊的⼆维数组和卷积核的⼆维核数组做互相关运算，再将这ci个互相关运算的⼆维输出按通道相加，得到⼀个⼆维数组。这就是含多个通道的输⼊数据与多输⼊通道的卷积核做⼆维互相关运算的输出。

设卷积核输⼊通道数和输出通道数分别为ci和co，⾼和宽分别为kh和kw。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为ci × kh × kw的核数组。将它们在输出通道维上连结，卷积核的形状即co × ci × kh × kw。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输⼊数组计算而来。

1x1卷积层

1 × 1卷积失去了卷积层可以识别⾼和宽维度上相邻元素构成的模式的功能。通过1x1卷积层实现通道数的变换。假设我们将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那么1 × 1卷积层的作⽤与全连接层等价。

1 × 1卷积层通常⽤来调整⽹络层之间的通道数，并控制模型复杂度。

池化层：

池化层的提出是为了缓解卷积层对位置的过度敏感性。

卷积后，池化后尺寸计算公式：
(图像尺寸-卷积核尺寸 + 2\*填充值)/步长+1
(图像尺寸-池化窗尺寸 + 2\*填充值)/步长+1
(下取整)

在处理多通道输⼊数据时，池化层对每个输⼊通道分别池化

LeNet模型：
卷积层块⾥的基本单位是卷积层后接最⼤池化层：卷积层⽤来识别图像⾥的空间模式，如线条和物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数。第⼀个卷积层输出通道数为6，第⼆个卷积层输出通道数则增加到16。这是因为第⼆个卷积层⽐第⼀个卷积层的输⼊的⾼和宽要小，所以增加输出通道使两个卷积层的参数尺⼨类似。卷积层块的两个最⼤池化层的窗口形状均为2 × 2，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输⼊上每次滑动所覆盖的区域互不重叠。

AlexNet:
AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。⼀⽅⾯，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另⼀⽅⾯，ReLU激活函数在不同的参数初始化⽅法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度⼏乎为0，从而造成反向传播⽆法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到⼏乎为0的梯度，从而令模型⽆法得到有效训练。

VGG:
VGG块的组成规律是：连续使⽤数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。
VGG这种⾼和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺⼨和计算复杂度。

NiN:即串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。

使用1x1卷积层代替全连接层实现多个串联的维度问题
除使⽤NiN块以外，NiN还有⼀个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。这⾥的全局平均池化层即窗口形状等于输⼊空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺⼨，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。
• NiN重复使⽤由卷积层和代替全连接层的1 × 1卷积层构成的NiN块来构建深层⽹络。
• NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数
的NiN块和全局平均池化层。
• NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计。

GoogLeNet:
• Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层
来并⾏抽取信息，并使⽤1 × 1卷积层减少通道数从而降低模型复杂度。
• GoogLeNet将多个设计精细的Inception块和其他层串联起来。其中Inception块的通道数分
配之⽐是在ImageNet数据集上通过⼤量的实验得来的。
• GoogLeNet和它的后继者们⼀度是ImageNet上最⾼效的模型之⼀：在类似的测试精度下，
它们的计算复杂度往往更低。

批量归一化：   显著提高训练速度
批量归⼀化利⽤小批量上的均值和标准差，不断调整神经⽹络中间输出，从而使整个神经⽹络在各层的中间输出的数值更稳定。
• 在模型训练时，批量归⼀化利⽤小批量上的均值和标准差，不断调整神经⽹络的中间输出，从而使整个神经⽹络在各层的中间输出的数值更稳定。
• 对全连接层和卷积层做批量归⼀化的⽅法稍有不同。
• 批量归⼀化层和丢弃层⼀样，在训练模式和预测模式的计算结果是不⼀样的。
• Gluon提供的BatchNorm类使⽤起来简单、⽅便
其实如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的，因此我们引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布

ResNet：

DenseNet：稠密网络
DenseNet⾥模块B的输出不是像ResNet那样和模块A的输出相加，而是在通道维上连结。这样模块A的输出可以直接传⼊模块B后⾯的层。在这个设计⾥，模块A直接跟模块B后⾯的所有层连接在了⼀起。这也是它被称为“稠密连接”的原因。

循环神经网络：
通过马尔科夫链和N元语法来进行sequence处理
N元语法是基于n − 1阶⻢尔可夫链的概率语⾔模型，其中n权衡了计算复杂度和模型准确性

门控循环单元 GRU：
引⼊了重置⻔（reset gate）和更新⻔（update gate）的概念，从而修改了循环神经⽹络中隐藏状态的计算⽅式。

• 重置⻔有助于捕捉时间序列⾥短期的依赖关系；
• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。

• ⻔控循环神经⽹络可以更好地捕捉时间序列中时间步距离较⼤的依赖关系。
• ⻔控循环单元引⼊了⻔的概念，从而修改了循环神经⽹络中隐藏状态的计算⽅式。它包括
重置⻔、更新⻔、候选隐藏状态和隐藏状态。
• 重置⻔有助于捕捉时间序列⾥短期的依赖关系。
• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。

LSTM:

LSTM 中引⼊了3个⻔，即输⼊⻔（input gate）、遗忘⻔（forget gate）和输出⻔（output gate），
以及与隐藏状态形状相同的记忆细胞（某些⽂献把记忆细胞当成⼀种特殊的隐藏状态），从而记
录额外的信息。

