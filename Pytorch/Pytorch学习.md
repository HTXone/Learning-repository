# Pytorch学习

## NDArray

向量类 运算的单元

## 深度学习基础

线性回归：对变量间的线性关系进行拟合

损失函数：衡量模型预测与真实数据的偏差 在线性回归过程中，希望找到一组模型参数来使训练样本平均损失最小

优化算法：小批量随机梯度下降

• 和⼤多数深度学习模型⼀样，对于线性回归这样⼀种单层神经⽹络，它的基本要素包括模
型、训练数据、损失函数和优化算法。
• 既可以⽤神经⽹络图表⽰线性回归，⼜可以⽤⽮量计算表⽰该模型。
• 应该尽可能采⽤⽮量计算，以提升计算效率

在Gluon中，Sequential实例可以看作是⼀个串联各个层的容器。在构造模型时，我们在该容器中依次添加层。当给定输⼊数据时，容器中的每⼀层将依次计算并将输出作为下⼀层的输⼊。

在Gluon中，data模块提供了有关数据处理的⼯具，nn模块定义了⼤量神经⽹络的层，loss模块定义了各种损失函数

MXNet的initializer模块提供了模型参数初始化的各种⽅法。

softmax回归：预测离散分类问题的相似概率

与线性回归相比，将输出单元从一个变为多个，且引入softmax运算使输出更适合离散值的预测和训练

通过交叉熵来衡量两个概率分布差异 通过最小化交叉熵损失函数来最大化预测概率

• softmax回归适⽤于分类问题。它使⽤softmax运算输出类别的概率分布。
• softmax回归是⼀个单层神经⽹络，输出个数等于分类问题中的类别个数。
• 交叉熵适合衡量两个概率分布的差异。

## 多层感知机

结构：输入层，输出层，隐藏层
神经网络只是引入隐藏层 不加变化 则仅等价于单层神经网络

通过引入非线性的激活函数 改变网络价值

激活函数
ReLU：只保存正数 导数上0不可导，正数为1负数为0
Sigmoid：正数接近1 负数接近0 导数上越接近0导数越大
tanh：正数接近1 负数接近-1 导数上越接近0导数越大 与sigmoid相比更陡峭

## 模型选择：

训练误差与泛化误差：训练误差指模型在训练数据集上表现出的误差，泛化误差指的是任意测试数据集上表现出的误差期望

验证集：预留在训练集和测试集之外的数据进行模型选择（测试集严格上只在最后评价使用，但是一般测试集与验证集的界限模糊）

K折：原始旋连数据分割成k个不同子数据集，用K-1个训练 用1个验证，每次验证子数据集不同 取平均

欠拟合：模型无法得到较低的训练误差

过拟合：训练误差远小于在测试数据集上的误差

模型复杂度越低越容易出现欠拟合 模型复杂度越高 越容易出现过拟合（高阶多项式函数比低阶多项式函数更容易在相同训练数据上得到低的训练误差）

训练数据集大小 在训练集中样本过少时容易出现过拟合

由于⽆法从训练误差估计泛化误差，⼀味地降低训练误差并不意味着泛化误差⼀定会降低。
机器学习模型应关注降低泛化误差。可以使⽤验证数据集来进⾏模型选择。
⽋拟合指模型⽆法得到较低的训练误差，过拟合指模型的训练误差远小于它在测试数据集上的误差。应选择复杂度合适的模型并避免使⽤过少的训练样本。

通过权重衰减来减少过拟合问题(L2范数正则化)

L2范数正则化在模型原损失函数基础上添加L2范数惩罚项，从而得到训练所需要最小化的函数。
L2范数惩罚项指的是模型权重参数每个元素的平⽅和与⼀个正的常数的乘积。

权重衰减通过惩罚绝对值较⼤的模型参数为需要学习的模型增加了限制

• 正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常⽤⼿
段。
• 权重衰减等价于L2范数正则化，通常会使学到的权重参数的元素较接近0。
• 权重衰减可以通过Gluon的wd超参数来指定。
• 可以定义多个Trainer实例对不同的模型参数使⽤不同的迭代⽅法

通过丢弃法解决过拟合问题

通过概率丢弃层单元从而在训练模型时起到正则化的作⽤，并可以⽤来应对过拟合

但在测试过程中为保障确定的结果启用所有节点

正向传播 正向传播是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）。

反向传播 反向传播指的是计算神经⽹络参数梯度的⽅法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输⼊层的顺序，依次计算并存储⽬标函数有关神经⽹络各层的中间变量以及参数的梯度。

正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。

反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的

在模型参数初始化完成后，我们交替地进⾏正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。

当神经⽹络的层数较多时，模型的数值稳定性容易变差。

解决梯度衰减和爆炸问题：随机初始化模型参数

通过使用Block来实现更复杂的模型

对于使⽤Sequential类构造的神经⽹络，我们可以通过⽅括号[]来访问⽹络的任⼀层

对于Sequential实例中含模型参数的层，我们可以通过Block类的params属性来访问该层包含的所有参数。

Gluon⾥参数类型为Parameter类，它包含参数和梯度的数值，可以分别通过data函数和grad函数来访问。

可以使⽤collect_params函数来获取net变量所有嵌套（例如通过add函数嵌套）
的层所包含的所有参数。它返回的同样是⼀个由参数名称到参数实例的字典。

通过NDArray的context属性来查看该NDArray所在的设备。

如果源变量和⽬标变量的context⼀致，as_in_context函数使⽬标变量和源
变量共享源变量的内存或显存。而copyto函数总是为⽬标变量开新的内存或显存。

## 卷积神经网络

卷积层可通过重复使⽤卷积核有效地表征局部空间。

卷积运算与互相关运算：为了得到卷积运算的输出，我们只需将核数组左右翻转并
上下翻转，再与输⼊数组做互相关运算。

特征图核感受野：⼆维卷积层输出的⼆维数组可以看作是输⼊在空间维度（宽和⾼）上某⼀级的表征，也叫特征图（feature map）。影响元素x的前向计算的所有可能输⼊区域（可能⼤于输⼊的实际尺⼨）叫做x的感受野（receptive field）。

卷积输出：假设输⼊形状是n_h × n_w，卷积核窗口形状是k_h × k_w，那么输出形状将会是
(n_h − k_h + 1) × (n_w − k_w + 1).

padding：在⾼的两侧⼀共填充p_h⾏，在宽的两侧⼀共填充p_w列，那么输出形状将会是
(n_h − k_h + p_h + 1) × (n_w − k_w + p_w + 1),

在很多情况下，我们会设置ph = kh−1和pw = kw −1来使输⼊和输出具有相同的⾼和宽

假设这⾥kh是奇数，我们会在⾼的两侧分别填充ph/2⾏。如果kh是偶数，⼀种可能是在输⼊的顶端⼀侧填充⌈ph/2⌉⾏，而在底端⼀侧填充⌊ph/2⌋⾏。在宽的两侧填充同理。

步幅：当⾼上步幅为s_h，宽上步幅为s_w时，输出形状为
⌊(n_h − k_h + p_h + s_h)/s_h⌋ × ⌊(n_w − k_w + p_w + s_w)/s_w⌋.

如果设置p_h = k_h−1和p_w = k_w −1，那么输出形状将简化为⌊(n_h+s_h−1)/s_h⌋×⌊(n_w +s_w −1)/s_w⌋。更进⼀步，如果输⼊的⾼和宽能分别被⾼和宽上的步幅整除，那么输出形状将是(n_h/s_h)×(n_w/s_w)。

多通道：

假设输⼊数据的通道数为ci，那么卷积核的输⼊通道数同样为ci。设卷积核窗口形状为kh × kw。当ci = 1时，我们知道卷积核只包含⼀个形状为kh × kw的⼆维数组。当ci > 1时，我们将会为每个输⼊通道各分配⼀个形状为kh × kw的核数组。把这ci个数组在输⼊通道维上连结，即得到⼀个形状为ci × kh × kw的卷积核。由于输⼊和卷积核各有ci个通道，我们可以在各个通道上对输⼊的⼆维数组和卷积核的⼆维核数组做互相关运算，再将这ci个互相关运算的⼆维输出按通道相加，得到⼀个⼆维数组。这就是含多个通道的输⼊数据与多输⼊通道的卷积核做⼆维互相关运算的输出。

设卷积核输⼊通道数和输出通道数分别为ci和co，⾼和宽分别为kh和kw。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为ci × kh × kw的核数组。将它们在输出通道维上连结，卷积核的形状即co × ci × kh × kw。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输⼊数组计算而来。

1x1卷积层

1 × 1卷积失去了卷积层可以识别⾼和宽维度上相邻元素构成的模式的功能。通过1x1卷积层实现通道数的变换。假设我们将通道维当作特征维，将⾼和宽维度上的元素当成数据样本，那么1 × 1卷积层的作⽤与全连接层等价。

1 × 1卷积层通常⽤来调整⽹络层之间的通道数，并控制模型复杂度。

池化层：

池化层的提出是为了缓解卷积层对位置的过度敏感性。

卷积后，池化后尺寸计算公式：
(图像尺寸-卷积核尺寸 + 2\*填充值)/步长+1
(图像尺寸-池化窗尺寸 + 2\*填充值)/步长+1
(下取整)

在处理多通道输⼊数据时，池化层对每个输⼊通道分别池化

LeNet模型：
卷积层块⾥的基本单位是卷积层后接最⼤池化层：卷积层⽤来识别图像⾥的空间模式，如线条和物体局部，之后的最⼤池化层则⽤来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数。第⼀个卷积层输出通道数为6，第⼆个卷积层输出通道数则增加到16。这是因为第⼆个卷积层⽐第⼀个卷积层的输⼊的⾼和宽要小，所以增加输出通道使两个卷积层的参数尺⼨类似。卷积层块的两个最⼤池化层的窗口形状均为2 × 2，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输⼊上每次滑动所覆盖的区域互不重叠。

AlexNet:
AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。⼀⽅⾯，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另⼀⽅⾯，ReLU激活函数在不同的参数初始化⽅法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度⼏乎为0，从而造成反向传播⽆法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到⼏乎为0的梯度，从而令模型⽆法得到有效训练。

VGG:
VGG块的组成规律是：连续使⽤数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。
VGG这种⾼和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺⼨和计算复杂度。

NiN:即串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。

使用1x1卷积层代替全连接层实现多个串联的维度问题
除使⽤NiN块以外，NiN还有⼀个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。这⾥的全局平均池化层即窗口形状等于输⼊空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺⼨，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。
• NiN重复使⽤由卷积层和代替全连接层的1 × 1卷积层构成的NiN块来构建深层⽹络。
• NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数
的NiN块和全局平均池化层。
• NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计。

GoogLeNet:
• Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层
来并⾏抽取信息，并使⽤1 × 1卷积层减少通道数从而降低模型复杂度。
• GoogLeNet将多个设计精细的Inception块和其他层串联起来。其中Inception块的通道数分
配之⽐是在ImageNet数据集上通过⼤量的实验得来的。
• GoogLeNet和它的后继者们⼀度是ImageNet上最⾼效的模型之⼀：在类似的测试精度下，
它们的计算复杂度往往更低。

批量归一化：   显著提高训练速度
批量归⼀化利⽤小批量上的均值和标准差，不断调整神经⽹络中间输出，从而使整个神经⽹络在各层的中间输出的数值更稳定。
• 在模型训练时，批量归⼀化利⽤小批量上的均值和标准差，不断调整神经⽹络的中间输出，从而使整个神经⽹络在各层的中间输出的数值更稳定。
• 对全连接层和卷积层做批量归⼀化的⽅法稍有不同。
• 批量归⼀化层和丢弃层⼀样，在训练模式和预测模式的计算结果是不⼀样的。
• Gluon提供的BatchNorm类使⽤起来简单、⽅便
其实如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的，因此我们引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布

ResNet：

DenseNet：稠密网络
DenseNet⾥模块B的输出不是像ResNet那样和模块A的输出相加，而是在通道维上连结。这样模块A的输出可以直接传⼊模块B后⾯的层。在这个设计⾥，模块A直接跟模块B后⾯的所有层连接在了⼀起。这也是它被称为“稠密连接”的原因。

## 循环神经网络：

通过马尔科夫链和N元语法来进行sequence处理
N元语法是基于n − 1阶⻢尔可夫链的概率语⾔模型，其中n权衡了计算复杂度和模型准确性

通过隐藏层中保存的上次预测状态和输入来进行预测
H~t~ = ϕ(X~t~W~xh~ + H~t−1~W~hh~ + b~h~)
![](D:\Learn\QQ截图20210617152528.png)

通过梯度裁剪防止训练过程中梯度爆炸发生

使用困惑度评价语言模型的好坏，困惑度是对交叉熵损失函数做指数运算后得到的值

循环网络的梯度计算：时间反向传播

门控循环单元 GRU：
引⼊了重置⻔（reset gate）和更新⻔（update gate）的概念，从而修改了循环神经⽹络中隐藏状态的计算⽅式。

• 重置⻔有助于捕捉时间序列⾥短期的依赖关系；
• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。

Rt = σ(XtWxr + Ht−1Whr + br),
Zt = σ(XtWxz + Ht−1Whz + bz),

候选隐藏状态：将当前时间步重置⻔的输出与上⼀时间步隐藏状态做按元素乘法（符号为⊙）。如果重置⻔中元素值接近0，那么意味着重置对应隐藏状态元素为0，即丢弃上⼀时间步的隐藏状态。如果元素值接近1，那么表⽰保留上⼀时间步的隐藏状态。然后，将按元素乘法的结果与当前时间步的输⼊连结，再通过含激活函数tanh的全连接层计算出候选隐藏状态，其所有元素的值域为[−1, 1]

\~H~t~ = tanh(XtWxh + (Rt ⊙ Ht−1)Whh + bh)

时间步t的隐藏状态Ht ∈ Rn×h的计算使⽤当前时间步的更新⻔Zt来对上⼀时间步的隐藏状态Ht−1和当前时间步的候选隐藏状态H˜t做组合

Ht = Zt ⊙ Ht−1 + (1 − Zt) ⊙ H˜t.

![](D:\Learn\QQ截图20210617190255.png)

• ⻔控循环神经⽹络可以更好地捕捉时间序列中时间步距离较⼤的依赖关系。
• ⻔控循环单元引⼊了⻔的概念，从而修改了循环神经⽹络中隐藏状态的计算⽅式。它包括
重置⻔、更新⻔、候选隐藏状态和隐藏状态。
• 重置⻔有助于捕捉时间序列⾥短期的依赖关系。
• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。

LSTM:

LSTM 中引⼊了3个⻔，即输⼊⻔（input gate）、遗忘⻔（forget gate）和输出⻔（output gate），
以及与隐藏状态形状相同的记忆细胞（某些⽂献把记忆细胞当成⼀种特殊的隐藏状态），从而记录额外的信息。

It = σ(XtWxi + Ht−1Whi + bi),   输入门
Ft = σ(XtWxf + Ht−1Whf + bf ),  遗忘门
Ot = σ(XtWxo + Ht−1Who + bo),  输出门

C˜t = tanh(XtWxc + Ht−1Whc + bc), 候选记忆细胞
Ct = Ft ⊙ Ct−1 + It ⊙ C˜t.    记忆细胞
Ht = Ot ⊙ tanh(Ct).  隐藏状态
![](D:\Learn\QQ截图20210617191422.png)

深度循环神经⽹络：每个隐藏状态不断传递⾄当前层的下⼀时间步和当前时间步的下⼀层。

双向循环神经网络：双向循环神经⽹络通过增加从后往前传递信息的隐藏层来更灵活地处理连续信息。

## 梯度下降算法：

小批量随机梯度下降：梯度下降使⽤整个训练数据集来计算梯度，因此它有时也被称为批量梯度下降（batch gradient descent）。而随机梯度下降在每次迭代中只随机采样⼀个样本来计算梯度。 小批量综合以上方法，随机选取小批量的样本进行梯度下降。

动量法：在动量法中，⾃变量在各个⽅向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个⽅向上是否⼀致。

AdaGrad算法：根据⾃变量在每个维度的梯度值的⼤小来调整各个维度上的学习率，从而避免统⼀的学习率难以适应所有维度的问题

RMSProp算法：对AdaGrad算法的改进

AdaDelta算法：取消学习率超参数，自行调节

Adam算法：在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均

性能优化：

符号式编程：
符号式编程通常在计算流程完全定义好后才被执⾏

符号式编程的程序需要下⾯3个步骤：
1. 定义计算流程；
2. 把计算流程编译成可执⾏的程序；
3. 给定输⼊，调⽤编译好的程序执⾏。

调⽤hybridize函数后模型计算性能会提升的⼀个原因。但它可能的问题在于，我们损失了写程序的灵活性。

异步计算：

wait_to_read函数、waitall函数、asnumpy函数、asscalar函数和print函数
会触发让前端等待后端计算结果的⾏为。这类函数通常称为同步函数。



## CV

图像增广：图像增⼴（image augmentation）技术通过对训练图像做⼀系列随机改变，来产⽣相似但⼜不同的训练样本，从而扩⼤训练数据集的规模。图像增⼴的另⼀种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提⾼模型的泛化能⼒。

迁移学习 微调：1. 在源数据集（如ImageNet数据集）上预训练⼀个神经⽹络模型，即源模型。2. 创建⼀个新的神经⽹络模型，即⽬标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适⽤于⽬标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在⽬标模型中不予采⽤。3.为⽬标模型添加⼀个输出⼤小为⽬标数据集类别个数的输出层，并随机初始化该层的模型参数。4.在⽬标数据集（如椅⼦数据集）上训练⽬标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。

锚框：⽬标检测算法通常会在输⼊图像中采样⼤量的区域，然后判断这些区域中是否包含我们感兴趣的⽬标，并调整区域边缘从而更准确地预测⽬标的真实边界框（ground-truth bounding box）

锚框以每个像素为中⼼⽣成多个⼤小和宽⾼⽐（aspect ratio）不同的边界框。

通过图形框的并交集来评价边界框的拟合程度

锚框训练：

假设图像中锚框分别为A1, A2, . . . , Ana，真实边界框分别为B1, B2, . . . , Bnb，且na ≥ nb。定义矩阵X ∈ Rna×nb，其中第i⾏第j列的元素xij为锚框Ai与真实边界框Bj的交并⽐。⾸先，我们找出矩阵X中最⼤元素，并将该元素的⾏索引与列索引分别记为i1, j1。我们为锚框Ai1分配真实边界框Bj1。显然，锚框Ai1和真实边界框Bj1在所有的“锚框—真实边界框”的配对中相似度最⾼。接下来，将矩阵X中第i1⾏和第j1列上的所有元素丢弃。找出矩阵X中剩余的最⼤元素，并将该元素的⾏索引与列索引分别记为i2, j2。我们为锚框Ai2分配真实边界框Bj2，再将矩阵X中第i2⾏和第j2列上的所有元素丢弃。此时矩阵X中已有两⾏两列的元素被丢弃。依此类推，直到矩阵X中所有nb列元素全部被丢弃。这个时候，我们已为nb个锚框各分配了⼀个真实边界框。接下来，我们只遍历剩余的na − nb个锚框：给定其中的锚框Ai，根据矩阵X的第i⾏找到与Ai交并⽐最⼤的真实边界框Bj，且只有当该交并⽐⼤于预先设定的阈值时，才为锚框Ai分配真实边界框Bj。

通过非极大值抑制（NMS）来移除相似预测边界框：对于⼀个预测边界框B，模型会计算各个类别的预测概率。设其中最⼤的预测概率为p，该概率所对应的类别即B的预测类别。我们也将p称为预测边界框B的置信度。在同⼀图像上，我们将预测类别⾮背景的预测边界框按置信度从⾼到低排序，得到列表L。从L中选取置信度最⾼的预测边界框B1作为基准，将所有与B1的交并⽐⼤于某阈值的⾮基准预测边界框从L中移除。这⾥的阈值是预先设定的超参数。此时，L保留了置信度最⾼的预测边界框并移除了与其相似的其他预测边界框。接下来，从L中选取置信度第⼆⾼的预测边
界框B2作为基准，将所有与B2的交并⽐⼤于某阈值的⾮基准预测边界框从L中移除。重复这⼀过程，直到L中所有的预测边界框都曾作为基准。此时L中任意⼀对预测边界框的交并⽐都小于阈值。最终，输出列表L中的所有预测边界框。

单发多框检测SSD:它主要由⼀个基础⽹络块和若⼲个多尺度特征块串联而
成。其中基础⽹络块⽤来从原始图像中抽取特征，因此⼀般会选择常⽤的深度卷积神经⽹络。单发多框检测论⽂中选⽤了在分类层之前截断的VGG [1]，现在也常⽤ResNet替代。

语义分割 全卷积网络(FCN)： 全卷积⽹络先使⽤卷积神经⽹络抽取图像特征，然后通过1 × 1卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的⾼和宽变换为输⼊图像的尺⼨，从而输出每个像素的类别。

样式迁移：深度卷积神经⽹络凭借多个层逐级抽取图像的特征。我们可以选择其中某些层的输出作为内容特征或样式特征

样式迁移常⽤的损失函数由3部分组成：内容损失（contentloss）使合成图像与内容图像在内容特征上接近，样式损失（style loss）令合成图像与样式图像在样式特征上接近，而总变差损失（total variation loss）则有助于减少合成图像中的噪点。最后，当模型训练结束时，我们输出样式迁移的模型参数，即得到最终的合成图像。

## NLP

词嵌入word embedding：通过词嵌入的方式 把词映射为实数域向量的技术